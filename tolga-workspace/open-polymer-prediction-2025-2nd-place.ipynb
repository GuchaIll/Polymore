{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":12204996,"sourceType":"datasetVersion","datasetId":7688334},{"sourceId":12205277,"sourceType":"datasetVersion","datasetId":7688411},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162},{"sourceId":12235747,"sourceType":"datasetVersion","datasetId":7709500},{"sourceId":12330396,"sourceType":"datasetVersion","datasetId":7709869},{"sourceId":12419545,"sourceType":"datasetVersion","datasetId":7695386},{"sourceId":12501403,"sourceType":"datasetVersion","datasetId":7887657},{"sourceId":12600616,"sourceType":"datasetVersion","datasetId":7958862}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:23:37.562249Z","iopub.execute_input":"2025-08-21T18:23:37.562449Z","iopub.status.idle":"2025-08-21T18:23:39.239619Z","shell.execute_reply.started":"2025-08-21T18:23:37.562423Z","shell.execute_reply":"2025-08-21T18:23:39.238996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This cell skips the rest of the notebook except during competition scoring reruns, saving GPU quota.\n# Uncomment the buttom two lines to enable.\n\nfrom IPython import get_ipython\nfrom IPython.core.interactiveshell import ExecutionResult, ExecutionInfo\n\nimport os\n\nipython = get_ipython()\n\ndef no_op_run_cell(*args, **kwargs):\n    info = ExecutionInfo(\n        raw_cell=\"\",\n        store_history=False,\n        silent=True,\n        shell_futures=True,\n        cell_id=None\n    )\n    return ExecutionResult(info)\n\n#if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n#    ipython.run_cell = no_op_run_cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:23:39.241115Z","iopub.execute_input":"2025-08-21T18:23:39.241401Z","iopub.status.idle":"2025-08-21T18:23:39.245916Z","shell.execute_reply.started":"2025-08-21T18:23:39.241385Z","shell.execute_reply":"2025-08-21T18:23:39.245161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:23:39.246571Z","iopub.execute_input":"2025-08-21T18:23:39.246781Z","iopub.status.idle":"2025-08-21T18:23:44.874842Z","shell.execute_reply.started":"2025-08-21T18:23:39.246766Z","shell.execute_reply":"2025-08-21T18:23:44.87408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rdkit import Chem\nfrom rdkit.Chem import Descriptors, rdMolDescriptors, AllChem, Fragments, Lipinski\nfrom rdkit.Chem import rdmolops\n# Data paths\nBASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\nRDKIT_AVAILABLE = True\nTARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\ndef get_canonical_smiles(smiles):\n        \"\"\"Convert SMILES to canonical form for consistency\"\"\"\n        if not RDKIT_AVAILABLE:\n            return smiles\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n            if mol:\n                return Chem.MolToSmiles(mol, canonical=True)\n        except:\n            pass\n        return smiles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:23:44.875918Z","iopub.execute_input":"2025-08-21T18:23:44.876209Z","iopub.status.idle":"2025-08-21T18:23:45.173334Z","shell.execute_reply.started":"2025-08-21T18:23:44.876175Z","shell.execute_reply":"2025-08-21T18:23:45.172788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Cell 3: Robust Data Loading with Complete R-Group Filtering\n\"\"\"\nLoad competition data with complete filtering of problematic polymer notation\n\"\"\"\n\nprint(\"ðŸ“‚ Loading competition data...\")\ntrain = pd.read_csv(BASE_PATH + 'train.csv')\ntest = pd.read_csv(BASE_PATH + 'test.csv')\n\nprint(f\"   Training samples: {len(train)}\")\nprint(f\"   Test samples: {len(test)}\")\n\ndef clean_and_validate_smiles(smiles):\n    \"\"\"Completely clean and validate SMILES, removing all problematic patterns\"\"\"\n    if not isinstance(smiles, str) or len(smiles) == 0:\n        return None\n    \n    # List of all problematic patterns we've seen\n    bad_patterns = [\n        '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \n        \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n        # Additional patterns that cause issues\n        '([R])', '([R1])', '([R2])', \n    ]\n    \n    # Check for any bad patterns\n    for pattern in bad_patterns:\n        if pattern in smiles:\n            return None\n    \n    # Additional check: if it contains ] followed by [ without valid atoms, likely polymer notation\n    if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n        return None\n    \n    # Try to parse with RDKit if available\n    if RDKIT_AVAILABLE:\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n            if mol is not None:\n                return Chem.MolToSmiles(mol, canonical=True)\n            else:\n                return None\n        except:\n            return None\n    \n    # If RDKit not available, return cleaned SMILES\n    return smiles\n\n# Clean and validate all SMILES\nprint(\"ðŸ”„ Cleaning and validating SMILES...\")\ntrain['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\ntest['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n\n# Remove invalid SMILES\ninvalid_train = train['SMILES'].isnull().sum()\ninvalid_test = test['SMILES'].isnull().sum()\n\nprint(f\"   Removed {invalid_train} invalid SMILES from training data\")\nprint(f\"   Removed {invalid_test} invalid SMILES from test data\")\n\ntrain = train[train['SMILES'].notnull()].reset_index(drop=True)\ntest = test[test['SMILES'].notnull()].reset_index(drop=True)\n\nprint(f\"   Final training samples: {len(train)}\")\nprint(f\"   Final test samples: {len(test)}\")\n\ndef add_extra_data_clean(df_train, df_extra, target):\n    \"\"\"Add external data with thorough SMILES cleaning\"\"\"\n    n_samples_before = len(df_train[df_train[target].notnull()])\n    \n    print(f\"      Processing {len(df_extra)} {target} samples...\")\n    \n    # Clean external SMILES\n    df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n    \n    # Remove invalid SMILES and missing targets\n    before_filter = len(df_extra)\n    df_extra = df_extra[df_extra['SMILES'].notnull()]\n    df_extra = df_extra.dropna(subset=[target])\n    after_filter = len(df_extra)\n    \n    print(f\"      Kept {after_filter}/{before_filter} valid samples\")\n    \n    if len(df_extra) == 0:\n        print(f\"      No valid data remaining for {target}\")\n        return df_train\n    \n    # Group by canonical SMILES and average duplicates\n    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n    \n    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n\n    # Fill missing values\n    filled_count = 0\n    for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n        if smile in cross_smiles:\n            df_train.loc[df_train['SMILES']==smile, target] = \\\n                df_extra[df_extra['SMILES']==smile][target].values[0]\n            filled_count += 1\n    \n    # Add unique SMILES\n    extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n    if len(extra_to_add) > 0:\n        for col in TARGETS:\n            if col not in extra_to_add.columns:\n                extra_to_add[col] = np.nan\n        \n        extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n        df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n\n    n_samples_after = len(df_train[df_train[target].notnull()])\n    print(f'      {target}: +{n_samples_after-n_samples_before} samples, +{len(unique_smiles_extra)} unique SMILES')\n    return df_train\n\n# Load external datasets with robust error handling\nprint(\"\\nðŸ“‚ Loading external datasets...\")\n\nexternal_datasets = []\n\n# Function to safely load datasets\ndef safe_load_dataset(path, target, processor_func, description):\n    try:\n        if path.endswith('.xlsx'):\n            data = pd.read_excel(path)\n        else:\n            data = pd.read_csv(path)\n        \n        data = processor_func(data)\n        external_datasets.append((target, data))\n        print(f\"   âœ… {description}: {len(data)} samples\")\n        return True\n    except Exception as e:\n        print(f\"   âš ï¸ {description} failed: {str(e)[:100]}\")\n        return False\n\n# Load each dataset\nsafe_load_dataset(\n    '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n    'Tc',\n    lambda df: df.rename(columns={'TC_mean': 'Tc'}),\n    'Tc data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/external-polymer-data/TgSS_enriched_cleaned.csv',\n    'Tg', \n    lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,\n    'TgSS enriched data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',\n    'Tg',\n    lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),\n    'JCIM Tg data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/data_tg3.xlsx',\n    'Tg',\n    lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),\n    'Xlsx Tg data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',\n    'Density',\n    lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n                .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n                .assign(Density=lambda x: x['Density'].astype(float) - 0.118),\n    'Density data'\n)\n\n#safe_load_dataset(\n#    '/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv',\n#    'FFV', \n#    lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,\n#    'dataset 4'\n#)\n\n# Integrate external data\nprint(\"\\nðŸ”„ Integrating external data...\")\ntrain_extended = train[['SMILES'] + TARGETS].copy()\n\nfor target, dataset in external_datasets:\n    print(f\"   Processing {target} data...\")\n    train_extended = add_extra_data_clean(train_extended, dataset, target)\n\nprint(f\"\\nðŸ“Š Final training data:\")\nprint(f\"   Original samples: {len(train)}\")\nprint(f\"   Extended samples: {len(train_extended)}\")\nprint(f\"   Gain: +{len(train_extended) - len(train)} samples\")\n\nfor target in TARGETS:\n    count = train_extended[target].notna().sum()\n    original_count = train[target].notna().sum() if target in train.columns else 0\n    gain = count - original_count\n    print(f\"   {target}: {count:,} samples (+{gain})\")\n\nprint(f\"\\nâœ… Data integration complete with clean SMILES!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:23:45.174239Z","iopub.execute_input":"2025-08-21T18:23:45.17472Z","iopub.status.idle":"2025-08-21T18:24:03.876694Z","shell.execute_reply.started":"2025-08-21T18:23:45.174696Z","shell.execute_reply":"2025-08-21T18:24:03.875926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef separate_subtables(train_df):\n\t\n\tlabels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\tsubtables = {}\n\tfor label in labels:\n\t\tsubtables[label] = train_df[['SMILES', label]][train_df[label].notna()]\n\treturn subtables\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:24:03.877602Z","iopub.execute_input":"2025-08-21T18:24:03.878295Z","iopub.status.idle":"2025-08-21T18:24:03.882456Z","shell.execute_reply.started":"2025-08-21T18:24:03.878273Z","shell.execute_reply":"2025-08-21T18:24:03.881616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef augment_smiles_dataset(smiles_list, labels, num_augments=3):\n\t\"\"\"\n\tAugments a list of SMILES strings by generating randomized versions.\n\n\tParameters:\n\t\tsmiles_list (list of str): Original SMILES strings.\n\t\tlabels (list or np.array): Corresponding labels.\n\t\tnum_augments (int): Number of augmentations per SMILES.\n\n\tReturns:\n\t\ttuple: (augmented_smiles, augmented_labels)\n\t\"\"\"\n\taugmented_smiles = []\n\taugmented_labels = []\n\n\tfor smiles, label in zip(smiles_list, labels):\n\t\tmol = Chem.MolFromSmiles(smiles)\n\t\tif mol is None:\n\t\t\tcontinue\n\t\t# Add original\n\t\taugmented_smiles.append(smiles)\n\t\taugmented_labels.append(label)\n\t\t# Add randomized versions\n\t\tfor _ in range(num_augments):\n\t\t\trand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n\t\t\taugmented_smiles.append(rand_smiles)\n\t\t\taugmented_labels.append(label)\n\n\treturn augmented_smiles, np.array(augmented_labels)\n\nfrom rdkit.Chem import Descriptors, MACCSkeys\nfrom rdkit.Chem.rdMolDescriptors import CalcTPSA, CalcNumRotatableBonds\nfrom rdkit.Chem.Descriptors import MolWt, MolLogP\nfrom rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\n\nimport networkx as nx\ndef smiles_to_combined_fingerprints_with_descriptors(smiles_list, radius=2, n_bits=128):\n    generator = GetMorganGenerator(radius=radius, fpSize=n_bits)\n    atom_pair_gen = GetAtomPairGenerator(fpSize=n_bits)\n    torsion_gen = GetTopologicalTorsionGenerator(fpSize=n_bits)\n\n    fingerprints = []\n    descriptors = []\n    valid_smiles = []\n    invalid_indices = []\n\n    for i, smiles in enumerate(smiles_list):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol:\n            # Fingerprints\n            morgan_fp = generator.GetFingerprint(mol)\n            #atom_pair_fp = atom_pair_gen.GetFingerprint(mol)\n            #torsion_fp = torsion_gen.GetFingerprint(mol)\n            maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n\n            combined_fp = np.concatenate([\n                np.array(morgan_fp),\n                #np.array(atom_pair_fp),\n                #np.array(torsion_fp),\n                np.array(maccs_fp)\n            ])\n            fingerprints.append(combined_fp)\n\n            # RDKit Descriptors\n            descriptor_values = {}\n            for name, func in Descriptors.descList:\n                try:\n                    descriptor_values[name] = func(mol)\n                except:\n                    descriptor_values[name] = None\n\n            # Specific descriptors\n            descriptor_values['MolWt'] = MolWt(mol)\n            descriptor_values['LogP'] = MolLogP(mol)\n            descriptor_values['TPSA'] = CalcTPSA(mol)\n            descriptor_values['RotatableBonds'] = CalcNumRotatableBonds(mol)\n            descriptor_values['NumAtoms'] = mol.GetNumAtoms()\n            descriptor_values['SMILES'] = smiles\n\n            # Graph-based features\n            try:\n                adj = rdmolops.GetAdjacencyMatrix(mol)\n                G = nx.from_numpy_array(adj)\n\n                if nx.is_connected(G):\n                    descriptor_values['graph_diameter'] = nx.diameter(G)\n                    descriptor_values['avg_shortest_path'] = nx.average_shortest_path_length(G)\n                else:\n                    descriptor_values['graph_diameter'] = 0\n                    descriptor_values['avg_shortest_path'] = 0\n\n                descriptor_values['num_cycles'] = len(list(nx.cycle_basis(G)))\n            except:\n                descriptor_values['graph_diameter'] = None\n                descriptor_values['avg_shortest_path'] = None\n                descriptor_values['num_cycles'] = None\n\n            descriptors.append(descriptor_values)\n            valid_smiles.append(smiles)\n        else:\n            #fingerprints.append(np.zeros(n_bits * 3 + 167))\n            fingerprints.append(np.zeros(n_bits  + 167))\n            descriptors.append(None)\n            valid_smiles.append(None)\n            invalid_indices.append(i)\n\n    return np.array(fingerprints), descriptors, valid_smiles, invalid_indices\n\ndef smiles_to_combined_fingerprints_with_descriptorsOriginal(smiles_list, radius=2, n_bits=128):\n    generator = GetMorganGenerator(radius=radius, fpSize=n_bits)\n    atom_pair_gen = GetAtomPairGenerator(fpSize=n_bits)\n    torsion_gen = GetTopologicalTorsionGenerator(fpSize=n_bits)\n\n    fingerprints = []\n    descriptors = []\n    valid_smiles = []\n    invalid_indices = []\n\n    for i, smiles in enumerate(smiles_list):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol:\n            # Fingerprints\n            morgan_fp = generator.GetFingerprint(mol)\n            #atom_pair_fp = atom_pair_gen.GetFingerprint(mol)\n            #torsion_fp = torsion_gen.GetFingerprint(mol)\n            maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n\n            combined_fp = np.concatenate([\n                np.array(morgan_fp),\n                #np.array(atom_pair_fp),\n                #np.array(torsion_fp),\n                np.array(maccs_fp)\n            ])\n            fingerprints.append(combined_fp)\n\n            # All RDKit Descriptors\n            descriptor_values = {}\n            for name, func in Descriptors.descList:\n                try:\n                    descriptor_values[name] = func(mol)\n                except:\n                    descriptor_values[name] = None\n\n            # Add specific descriptors explicitly\n            descriptor_values['MolWt'] = MolWt(mol)\n            descriptor_values['LogP'] = MolLogP(mol)\n            descriptor_values['TPSA'] = CalcTPSA(mol)\n            descriptor_values['RotatableBonds'] = CalcNumRotatableBonds(mol)\n            descriptor_values['NumAtoms'] = mol.GetNumAtoms()\n            descriptor_values['SMILES'] = smiles\n            #descriptor_values['RadiusOfGyration'] =CalcRadiusOfGyration(mol)\n\n            descriptors.append(descriptor_values)\n            valid_smiles.append(smiles)\n        else:\n            #fingerprints.append(np.zeros(n_bits * 3 + 167))\n            fingerprints.append(np.zeros( 167))\n            descriptors.append(None)\n            valid_smiles.append(None)\n            invalid_indices.append(i)\n\n    return np.array(fingerprints), descriptors, valid_smiles, invalid_indices\n\ndef make_smile_canonical(smile): # To avoid duplicates, for example: canonical '*C=C(*)C' == '*C(=C*)C'\n\ttry:\n\t\tmol = Chem.MolFromSmiles(smile)\n\t\tcanon_smile = Chem.MolToSmiles(mol, canonical=True)\n\t\treturn canon_smile\n\texcept:\n\t\treturn np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:24:03.884818Z","iopub.execute_input":"2025-08-21T18:24:03.885016Z","iopub.status.idle":"2025-08-21T18:24:04.704939Z","shell.execute_reply.started":"2025-08-21T18:24:03.885001Z","shell.execute_reply":"2025-08-21T18:24:04.704153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom rdkit import Chem\nfrom rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\nfrom rdkit.Chem import MACCSkeys\nfrom rdkit.Chem import Descriptors\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, MACCSkeys, Descriptors\nfrom rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\nimport numpy as np\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_absolute_error","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:24:04.705689Z","iopub.execute_input":"2025-08-21T18:24:04.706085Z","iopub.status.idle":"2025-08-21T18:24:10.487744Z","shell.execute_reply.started":"2025-08-21T18:24:04.706067Z","shell.execute_reply":"2025-08-21T18:24:10.48696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#required_descriptors = {'MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n#required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path'}\nrequired_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n#required_descriptors = {}\n\nfilters = {\n    'Tg': list(set([\n        'BalabanJ','BertzCT','Chi1','Chi3n','Chi4n','EState_VSA4','EState_VSA8',\n        'FpDensityMorgan3','HallKierAlpha','Kappa3','MaxAbsEStateIndex','MolLogP',\n        'NumAmideBonds','NumHeteroatoms','NumHeterocycles','NumRotatableBonds',\n        'PEOE_VSA14','Phi','RingCount','SMR_VSA1','SPS','SlogP_VSA1','SlogP_VSA5',\n        'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState4','VSA_EState6','VSA_EState7',\n        'VSA_EState8','fr_C_O_noCOO','fr_NH1','fr_benzene','fr_bicyclic','fr_ether',\n        'fr_unbrch_alkane'\n    ]).union(required_descriptors)),\n\n    'FFV': list(set([\n        'AvgIpc','BalabanJ','BertzCT','Chi0','Chi0n','Chi0v','Chi1','Chi1n','Chi1v',\n        'Chi2n','Chi2v','Chi3n','Chi3v','Chi4n','EState_VSA10','EState_VSA5',\n        'EState_VSA7','EState_VSA8','EState_VSA9','ExactMolWt','FpDensityMorgan1',\n        'FpDensityMorgan2','FpDensityMorgan3','FractionCSP3','HallKierAlpha',\n        'HeavyAtomMolWt','Kappa1','Kappa2','Kappa3','MaxAbsEStateIndex',\n        'MaxEStateIndex','MinEStateIndex','MolLogP','MolMR','MolWt','NHOHCount',\n        'NOCount','NumAromaticHeterocycles','NumHAcceptors','NumHDonors',\n        'NumHeterocycles','NumRotatableBonds','PEOE_VSA14','RingCount','SMR_VSA1',\n        'SMR_VSA10','SMR_VSA3','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA9','SPS',\n        'SlogP_VSA1','SlogP_VSA10','SlogP_VSA11','SlogP_VSA12','SlogP_VSA2',\n        'SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7',\n        'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState10','VSA_EState2',\n        'VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7',\n        'VSA_EState8','VSA_EState9','fr_Ar_N','fr_C_O','fr_NH0','fr_NH1',\n        'fr_aniline','fr_ether','fr_halogen','fr_thiophene'\n    ]).union(required_descriptors)),\n\n    'Tc': list(set([\n        'BalabanJ','BertzCT','Chi0','EState_VSA5','ExactMolWt','FpDensityMorgan1',\n        'FpDensityMorgan2','FpDensityMorgan3','HeavyAtomMolWt','MinEStateIndex',\n        'MolWt','NumAtomStereoCenters','NumRotatableBonds','NumValenceElectrons',\n        'SMR_VSA10','SMR_VSA7','SPS','SlogP_VSA6','SlogP_VSA8','VSA_EState1',\n        'VSA_EState7','fr_NH1','fr_ester','fr_halogen'\n    ]).union(required_descriptors)),\n\n    'Density': list(set([\n        'BalabanJ','Chi3n','Chi3v','Chi4n','EState_VSA1','ExactMolWt',\n        'FractionCSP3','HallKierAlpha','Kappa2','MinEStateIndex','MolMR','MolWt',\n        'NumAliphaticCarbocycles','NumHAcceptors','NumHeteroatoms',\n        'NumRotatableBonds','SMR_VSA10','SMR_VSA5','SlogP_VSA12','SlogP_VSA5',\n        'TPSA','VSA_EState10','VSA_EState7','VSA_EState8'\n    ]).union(required_descriptors)),\n\n    'Rg': list(set([\n        'AvgIpc','Chi0n','Chi1v','Chi2n','Chi3v','ExactMolWt','FpDensityMorgan1',\n        'FpDensityMorgan2','FpDensityMorgan3','HallKierAlpha','HeavyAtomMolWt',\n        'Kappa3','MaxAbsEStateIndex','MolWt','NOCount','NumRotatableBonds',\n        'NumUnspecifiedAtomStereoCenters','NumValenceElectrons','PEOE_VSA14',\n        'PEOE_VSA6','SMR_VSA1','SMR_VSA5','SPS','SlogP_VSA1','SlogP_VSA2',\n        'SlogP_VSA7','SlogP_VSA8','VSA_EState1','VSA_EState8','fr_alkyl_halide',\n        'fr_halogen'\n    ]).union(required_descriptors))\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:24:10.488563Z","iopub.execute_input":"2025-08-21T18:24:10.489027Z","iopub.status.idle":"2025-08-21T18:24:10.49922Z","shell.execute_reply.started":"2025-08-21T18:24:10.489001Z","shell.execute_reply":"2025-08-21T18:24:10.498402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.mixture import GaussianMixture\n\ndef augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n    \"\"\"\n    Augments a dataset using Gaussian Mixture Models.\n\n    Parameters:\n    - X: pd.DataFrame or np.ndarray â€” feature matrix\n    - y: pd.Series or np.ndarray â€” target values\n    - n_samples: int â€” number of synthetic samples to generate\n    - n_components: int â€” number of GMM components\n    - random_state: int â€” random seed for reproducibility\n\n    Returns:\n    - X_augmented: pd.DataFrame â€” augmented feature matrix\n    - y_augmented: pd.Series â€” augmented target values\n    \"\"\"\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n    elif not isinstance(X, pd.DataFrame):\n        raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n\n    X.columns = X.columns.astype(str)\n\n    if isinstance(y, np.ndarray):\n        y = pd.Series(y)\n    elif not isinstance(y, pd.Series):\n        raise ValueError(\"y must be a pandas Series or a NumPy array\")\n\n    df = X.copy()\n    df['Target'] = y.values\n\n    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n    gmm.fit(df)\n\n    synthetic_data, _ = gmm.sample(n_samples)\n    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n\n    augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n\n    X_augmented = augmented_df.drop(columns='Target')\n    y_augmented = augmented_df['Target']\n\n    return X_augmented, y_augmented\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:24:10.500028Z","iopub.execute_input":"2025-08-21T18:24:10.500527Z","iopub.status.idle":"2025-08-21T18:24:10.983188Z","shell.execute_reply.started":"2025-08-21T18:24:10.500505Z","shell.execute_reply":"2025-08-21T18:24:10.982379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.ensemble import RandomForestRegressor\n\n\ntrain_df=train_extended\ntest_df=test\nsubtables = separate_subtables(train_df)\n\ntest_smiles = test_df['SMILES'].tolist()\ntest_ids = test_df['id'].values\nlabels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n#labels = ['Tc']\n\noutput_df = pd.DataFrame({\n\t'id': test_ids\n})\n\n\nfor label in labels:\n\tprint(f\"Processing label: {label}\")\n\tprint(subtables[label].head())\n\tprint(subtables[label].shape)\n\toriginal_smiles = subtables[label]['SMILES'].tolist()\n\toriginal_labels = subtables[label][label].values\n\n\toriginal_smiles, original_labels = augment_smiles_dataset(original_smiles, original_labels, num_augments=1)\n\tfingerprints, descriptors, valid_smiles, invalid_indices\\\n\t\t=smiles_to_combined_fingerprints_with_descriptors(original_smiles, radius=2, n_bits=128)\n\t# descriptors, valid_smiles, invalid_indices\\\n\t#\t =smiles_to_descriptors_with_fingerprints(original_smiles, radius=2, n_bits=128)\n\n\tX=pd.DataFrame(descriptors)\n\tX=X.drop(['BCUT2D_MWLOW','BCUT2D_MWHI','BCUT2D_CHGHI','BCUT2D_CHGLO','BCUT2D_LOGPHI','BCUT2D_LOGPLOW','BCUT2D_MRLOW','BCUT2D_MRHI','MinAbsPartialCharge','MaxPartialCharge','MinPartialCharge','MaxAbsPartialCharge', 'SMILES'],axis=1)\n\ty = np.delete(original_labels, invalid_indices)\n\t\n\t# pd.DataFrame(X).to_csv(f\"./mats/{label}.csv\")\n\t# pd.DataFrame(y).to_csv(f\"./mats/{label}label.csv\", header=None)\n\t\n\t# binned = pd.qcut(y, q=10, labels=False, duplicates='drop')\n\t# pd.DataFrame(binned).to_csv(f\"./mats/{label}integerlabel.csv\", header=None, index=False)\n\tX = X.filter(filters[label])\n\t# Convert fingerprints array to DataFrame\n\tfp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])\n\n\tprint(fp_df.shape)\n\t# Reset index to align with X\n\tfp_df.reset_index(drop=True, inplace=True)\n\tX.reset_index(drop=True, inplace=True)\n\t# Concatenate descriptors and fingerprints\n\tX = pd.concat([X, fp_df], axis=1)\n    \n\tprint(f\"After concat: {X.shape}\")\n\t\n\t# Set the variance threshold\n\tthreshold = 0.01\n\n\t# Apply VarianceThreshold\n\tselector = VarianceThreshold(threshold=threshold)\n\t\n\tX = selector.fit_transform(X)\n\n\tprint(f\"After variance cut: {X.shape}\")\n\n\t# Assuming you have X and y loaded\n    \n\tn_samples = 1000\n\n\tX, y = augment_dataset(X, y, n_samples=n_samples)\n\tprint(f\"After augment cut: {X.shape}\")\n\n\n\tX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=10)\n\t\n\tif label==\"Tg\":\n\t\tModel= XGBRegressor(n_estimators= 2173, learning_rate= 0.0672418745539774, max_depth= 6, reg_lambda= 5.545520219149715)\n\tif label=='Rg':\n\t\tModel = XGBRegressor(n_estimators= 520, learning_rate= 0.07324113948440986, max_depth= 5, reg_lambda=0.9717380315982088)\n\tif label=='FFV':\n# Best parameters found: {'n_estimators': 2202, 'learning_rate': 0.07220580588586338, 'max_depth': 4, 'reg_lambda': 2.8872976032666493}\n\t\tModel = XGBRegressor(n_estimators= 2202, learning_rate= 0.07220580588586338, max_depth= 4, reg_lambda= 2.8872976032666493)\n\tif label=='Tc':\n\t\tModel = XGBRegressor(n_estimators= 1488, learning_rate= 0.010456188013762864, max_depth= 5, reg_lambda= 9.970345982204618)\n#Best parameters found: {'n_estimators': 1488, 'learning_rate': 0.010456188013762864, 'max_depth': 5, 'reg_lambda': 9.970345982204618}\n\tif label=='Density':\n\t\tModel = XGBRegressor(n_estimators= 1958, learning_rate= 0.10955287548172478, max_depth= 5, reg_lambda= 3.074470087965767)\n\n\tRFModel=RandomForestRegressor(random_state=42)\n\tModel.fit(X_train,y_train)\n\tRFModel.fit(X_train,y_train)\n\ty_pred=Model.predict(X_test)\n\tprint(mean_absolute_error(y_pred,y_test))\n\n\tModel.fit(X,y)\n\tRFModel.fit(X,y)\n\t# Predict on test set\n\t#test_smiles = test_df['SMILES'].str.replace('*', 'C')\n\n\tfingerprints, descriptors, valid_smiles, invalid_indices\\\n\t\t=smiles_to_combined_fingerprints_with_descriptors(test_smiles, radius=2, n_bits=128)\n\ttest=pd.DataFrame(descriptors)\n\ttest=test.drop(['BCUT2D_MWLOW','BCUT2D_MWHI','BCUT2D_CHGHI','BCUT2D_CHGLO','BCUT2D_LOGPHI','BCUT2D_LOGPLOW','BCUT2D_MRLOW','BCUT2D_MRHI','MinAbsPartialCharge','MaxPartialCharge','MinPartialCharge','MaxAbsPartialCharge', 'SMILES'],axis=1)\n\n\ttest = test.filter(filters[label])\n    # Convert fingerprints array to DataFrame\n\tfp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])\n    \n\t# Reset index to align with X\n\tfp_df.reset_index(drop=True, inplace=True)\n\ttest.reset_index(drop=True, inplace=True)\n    # Concatenate descriptors and fingerprints\n\ttest = pd.concat([test, fp_df], axis=1)\n\ttest = selector.transform(test)\n\tprint(test.shape)\n\n\ty_pred1=Model.predict(test).flatten()\n\ty_pred2=RFModel.predict(test).flatten()\n\ty_pred=y_pred1*.6+y_pred2*.4\n\tprint(y_pred)\n\n\n\tnew_column_name = label\n\toutput_df[new_column_name] = y_pred\n\nprint(output_df)\n\n\noutput_df.to_csv('submission1.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:24:10.98402Z","iopub.execute_input":"2025-08-21T18:24:10.984275Z","execution_failed":"2025-08-21T18:34:32.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T18:34:32.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport joblib\nfrom transformers import PreTrainedModel, AutoConfig, BertModel, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer\nfrom sklearn.metrics import mean_absolute_error\nfrom torch import nn\nfrom transformers.activations import ACT2FN\nfrom tqdm import tqdm\nimport numpy as np\n\nclass ContextPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        pooler_size = getattr(config, 'pooler_hidden_size', config.hidden_size)\n        self.dense = nn.Linear(pooler_size, pooler_size)\n        \n        dropout_prob = getattr(config, 'pooler_dropout', config.hidden_dropout_prob)\n        self.dropout = nn.Dropout(dropout_prob)\n        \n        self.activation = getattr(config, 'pooler_hidden_act', config.hidden_act)\n        self.config = config\n\n    def forward(self, hidden_states):\n        context_token = hidden_states[:, 0] # CLS token\n        context_token = self.dropout(context_token)\n        pooled_output = self.dense(context_token)\n        pooled_output = ACT2FN[self.activation](pooled_output)\n        return pooled_output\n\nclass CustomModel(PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.backbone = AutoModel.from_config(config)\n        \n        self.pooler = ContextPooler(config)\n\n        pooler_output_dim = getattr(config, 'pooler_hidden_size', config.hidden_size)\n        self.output = torch.nn.Linear(pooler_output_dim, 1) # Still predicting one label at a time. Kinda stupid\n\n    def forward(\n        self,\n        input_ids,\n        scaler,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        labels=None,\n    ):\n        outputs = self.backbone(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n        )\n\n        pooled_output = self.pooler(outputs.last_hidden_state)\n        \n        # Final regression output\n        regression_output = self.output(pooled_output)\n\n        loss = None\n        true_loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n\n            unscaled_labels = scaler.inverse_transform(labels.cpu().numpy())\n            unscaled_outputs = scaler.inverse_transform(regression_output.cpu().detach().numpy())\n            \n            loss = loss_fn(regression_output, labels)\n            true_loss = mean_absolute_error(unscaled_outputs, unscaled_labels)\n\n        return {\n            \"loss\": loss,\n            \"logits\": regression_output,\n            \"true_loss\": true_loss\n        }","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T18:34:32.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 16\n\ndef tokenize_smiles(seq):\n    seq = [tokenizer.cls_token + smiles for smiles in seq] # If we pass a string, tokenizer will smartly think we want to create a sequence for each symbol\n    tokenized = tokenizer(seq, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n    return tokenized\n\ndef load_model(path):\n    config = AutoConfig.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\n    model = CustomModel(config).cuda()\n    checkpoint = torch.load(model_path)\n    model.load_state_dict(checkpoint)\n    return model\n\n\ndef make_predictions(model, scaler, smiles_seq):\n    aggregated_preds = []\n    for smiles in smiles_seq:\n        smiles = [smiles]\n        smiles_tokenized = tokenize_smiles(smiles)\n\n        input_ids = smiles_tokenized['input_ids'].cuda()\n        attention_mask = smiles_tokenized['attention_mask'].cuda()\n        with torch.no_grad():\n            preds = model(input_ids=input_ids, scaler=scaler, attention_mask=attention_mask)['logits'].cpu().numpy()\n        \n        true_preds = scaler.inverse_transform(preds).flatten()\n        aggregated_preds.append(true_preds.tolist())\n    return np.array(aggregated_preds)\n\n\ntest = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\ntest_copy = test.copy()\n\nsmiles_test = test['SMILES'].to_list()\n\ntargets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\nscalers = joblib.load('/kaggle/input/smiles-bert-models/target_scalers.pkl')\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T18:34:32.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom rdkit import Chem\nimport random\nfrom typing import Optional, List, Union\n\ndef augment_smiles_dataset(df: pd.DataFrame,\n                               smiles_column: str = 'SMILES',\n                               augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum'],\n                               n_augmentations: int = 100,\n                               preserve_original: bool = True,\n                               random_seed: Optional[int] = None) -> pd.DataFrame:\n    if random_seed is not None:\n        random.seed(random_seed)\n        np.random.seed(random_seed)\n    \n    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n            if mol is None:\n                return [smiles]\n            \n            augmented = []\n            \n            if strategy == 'enumeration':\n                # Standard SMILES enumeration\n                for _ in range(n_augmentations):\n                    enum_smiles = Chem.MolToSmiles(mol, \n                                                 canonical=False, \n                                                 doRandom=True,\n                                                 isomericSmiles=True)\n                    augmented.append(enum_smiles)\n            \n            elif strategy == 'kekulize':\n                # Kekulization variants\n                try:\n                    Chem.Kekulize(mol)\n                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n                    augmented.append(kek_smiles)\n                except:\n                    pass\n            \n            elif strategy == 'stereo_enum':\n                # Stereochemistry enumeration\n                for _ in range(n_augmentations // 2):\n                    # Remove stereochemistry\n                    Chem.RemoveStereochemistry(mol)\n                    no_stereo = Chem.MolToSmiles(mol)\n                    augmented.append(no_stereo)\n            \n            return list(set(augmented))  # Remove duplicates\n            \n        except Exception as e:\n            print(f\"Error in {strategy} for {smiles}: {e}\")\n            return [smiles]\n    \n    augmented_rows = []\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        original_smiles = row[smiles_column]\n        \n        if preserve_original:\n            original_row = row.to_dict()\n            original_row['augmentation_strategy'] = 'original'\n            original_row['is_augmented'] = False\n            augmented_rows.append(original_row)\n        \n        for strategy in augmentation_strategies:\n            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n            \n            for aug_smiles in strategy_smiles:\n                if aug_smiles != original_smiles:\n                    new_row = row.to_dict().copy()\n                    new_row[smiles_column] = aug_smiles\n                    new_row['augmentation_strategy'] = strategy\n                    new_row['is_augmented'] = True\n                    augmented_rows.append(new_row)\n    \n    augmented_df = pd.DataFrame(augmented_rows)\n    augmented_df = augmented_df.reset_index(drop=True)\n    \n    print(f\"Original size: {len(df)}, Augmented size: {len(augmented_df)}\")\n    print(f\"Augmentation factor: {len(augmented_df) / len(df):.2f}x\")\n    \n    return augmented_df\n\ntest = augment_smiles_dataset(test)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T18:34:32.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_mapping = {}\n\nfor i in tqdm(range(len(targets))):\n    target = targets[i]\n    scaler = scalers[i]\n\n    model_path = f'/kaggle/input/private-smile-bert-models/warm_smiles_model_{target}_target.pth' # Not actually warm\n    \n    model = load_model(model_path)\n    true_preds = []\n\n    for i, data in test.groupby('id'):\n        test_smiles = data['SMILES'].to_list()\n        augmented_preds = make_predictions(model, scaler, test_smiles)\n    \n        average_pred = np.median(augmented_preds)\n    \n        true_preds.append(float(average_pred.flatten()[0]))\n\n    preds_mapping[target] = true_preds","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T18:34:32.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame(preds_mapping)\nsubmission['id'] = test_copy['id']\nsubmission.to_csv('submission2.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T18:34:32.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom torch import nn\nimport seaborn as sns\nimport networkx as nx\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import rdmolops\nfrom rdkit.Chem import Descriptors\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n\n\nclass EnhancedEmbedding(nn.Module):\n    def __init__(self, categories, num_continuous, embedding_dim):\n        super().__init__()\n        self.cat_embeddings = nn.ModuleList([\n            nn.Embedding(num_cat + 1, embedding_dim) for num_cat in categories\n        ])\n        self.cont_emb = nn.Linear(num_continuous, embedding_dim) if num_continuous > 0 else None\n\n        self.feature_type_embed = nn.Embedding(len(categories) + (1 if num_continuous > 0 else 0), embedding_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n\n        for emb in self.cat_embeddings:\n            nn.init.xavier_uniform_(emb.weight)\n        if self.cont_emb:\n            nn.init.xavier_uniform_(self.cont_emb.weight)\n\n    def forward(self, x_cat, x_cont):\n        B = x_cat.size(0)\n        cat_tokens = torch.stack([emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeddings)], dim=1)\n        type_indices_cat = torch.arange(len(self.cat_embeddings), device=x_cat.device).unsqueeze(0).expand(B, -1)\n        cat_tokens += self.feature_type_embed(type_indices_cat)\n\n        if x_cont is not None and self.cont_emb:\n            cont_token = self.cont_emb(x_cont).unsqueeze(1)\n            cont_token += self.feature_type_embed(torch.full((B, 1), len(self.cat_embeddings), device=x_cat.device))\n            tokens = torch.cat([cat_tokens, cont_token], dim=1)\n        else:\n            tokens = cat_tokens\n\n        cls_tokens = self.cls_token.expand(B, 1, -1)\n        tokens = torch.cat([cls_tokens, tokens], dim=1)\n        return tokens\n\nclass GatedTransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.gate = nn.Linear(embed_dim * 2, embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, ff_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(ff_hidden_dim, embed_dim)\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        attn_out, _ = self.attn(x, x, x)\n        gate_input = torch.cat([x, attn_out], dim=-1)\n        gated_output = torch.sigmoid(self.gate(gate_input))\n        x = self.norm1((1 - gated_output) * x + gated_output * attn_out)\n        x = self.norm2(x + self.ff(x))\n        return x\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.attn_score = nn.Linear(embed_dim, 1)\n\n    def forward(self, x):\n        weights = torch.softmax(self.attn_score(x), dim=1)\n        return (weights * x).sum(dim=1)\n\nclass EnhancedFTTransformer(nn.Module):\n    def __init__(self, categories, num_continuous, embedding_dim, num_heads,\n                 num_layers, ff_hidden_dim, dropout=0.1, output_dim=1):\n        super().__init__()\n        self.embedding = EnhancedEmbedding(categories, num_continuous, embedding_dim)\n\n        self.transformer_blocks = nn.Sequential(*[\n            GatedTransformerBlock(embedding_dim, num_heads, ff_hidden_dim, dropout)\n            for _ in range(num_layers)\n        ])\n\n        self.attn_pooling = AttentionPooling(embedding_dim)\n        self.output_layer = nn.Sequential(\n            nn.Linear(embedding_dim * 3, ff_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(ff_hidden_dim, output_dim)\n        )\n\n    def forward(self, x_cat, x_cont):\n        tokens = self.embedding(x_cat, x_cont)  # Shape: [B, T, D]\n        tokens = self.transformer_blocks(tokens)\n\n        cls = tokens[:, 0]\n        mean_pool = tokens.mean(dim=1)\n        max_pool = tokens.max(dim=1)[0]\n\n        fused = torch.cat([cls, mean_pool, max_pool], dim=1)\n        return self.output_layer(fused).squeeze()\n\n\n# --- Configuration ---\nclass CFG:\n    \"\"\"\n    Configuration class for defining global parameters.\n    \"\"\"\n    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    SEED = 42\n    FOLDS = 5\n    PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n    TC_SMILES_PATH = '/kaggle/input/tc-smiles/Tc_SMILES.csv'\n    JCIM_SMILES_PATH = '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv'\n    DATA_TG3_PATH = '/kaggle/input/smiles-extra-data/data_tg3.xlsx'\n    DATA_DNST1_PATH = '/kaggle/input/smiles-extra-data/data_dnst1.xlsx'\n    NULL_FOR_SUBMISSION = -9999\n\n    # TabTransformer specific parameters\n    # TabTransformer specific parameters\"\n    EMBEDDING_DIM = 32 \n    NUM_HEADS = 2 \n    NUM_ENCODERS = 1\n    FF_HIDDEN_DIM = 128 \n    DROPOUT = 0.15\n    TAB_LEARNING_RATE = 0.001\n    TAB_EPOCHS = 100\n    TAB_BATCH_SIZE = 32\n    TAB_EARLY_STOPPING_PATIENCE = 15\n    WEIGHT_DECAY = 0.001\n    NUM_LAYERS = 2\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    LR_DECAY_RATE = 0.9658825\n\n\n# --- Data Loading ---\ndef load_data():\n    \"\"\"\n    Loads training and testing datasets.\n    \"\"\"\n    train_df = pd.read_csv(CFG.PATH + 'train.csv')\n    test_df = pd.read_csv(CFG.PATH + 'test.csv')\n    return train_df, test_df\n\n# --- SMILES Canonicalization ---\ndef make_smile_canonical(smile):\n    \"\"\"\n    Converts a SMILES string to its canonical form to ensure uniqueness.\n    Returns np.nan if conversion fails.\n    \"\"\"\n    try:\n        mol = Chem.MolFromSmiles(smile)\n        if mol is None:\n            return np.nan\n        canon_smile = Chem.MolToSmiles(mol, canonical=True)\n        return canon_smile\n    except Exception:\n        return np.nan\n\n# --- Extra Data Loading and Preprocessing ---\ndef load_and_preprocess_extra_data():\n    \"\"\"\n    Loads and preprocesses various external datasets.\n    \"\"\"\n    data_tc = pd.read_csv(CFG.TC_SMILES_PATH).rename(columns={'TC_mean': 'Tc'})\n    data_tc['SMILES'] = data_tc['SMILES'].apply(make_smile_canonical)\n    data_tc = data_tc.groupby('SMILES', as_index=False)['Tc'].mean() \n\n    data_tg2 = pd.read_csv(CFG.JCIM_SMILES_PATH, usecols=['SMILES', 'Tg (C)']).rename(columns={'Tg (C)': 'Tg'})\n    data_tg2['SMILES'] = data_tg2['SMILES'].apply(make_smile_canonical)\n    data_tg2 = data_tg2.groupby('SMILES', as_index=False)['Tg'].mean() \n\n    data_tg3 = pd.read_excel(CFG.DATA_TG3_PATH).rename(columns={'Tg [K]': 'Tg'})\n    data_tg3['Tg'] = data_tg3['Tg'] - 273.15\n    data_tg3['SMILES'] = data_tg3['SMILES'].apply(make_smile_canonical)\n    data_tg3 = data_tg3.groupby('SMILES', as_index=False)['Tg'].mean() \n\n    data_dnst = pd.read_excel(CFG.DATA_DNST1_PATH).rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n    data_dnst['SMILES'] = data_dnst['SMILES'].apply(make_smile_canonical)\n    data_dnst = data_dnst[(data_dnst['SMILES'].notnull()) & (data_dnst['Density'].notnull()) & (data_dnst['Density'] != 'nylon')]\n    data_dnst['Density'] = data_dnst['Density'].astype('float64')\n    data_dnst['Density'] -= 0.118\n    data_dnst = data_dnst.groupby('SMILES', as_index=False)['Density'].mean() \n\n    return data_tc, data_tg2, data_tg3, data_dnst\n\ndef add_extra_data(df_main, df_extra, target):\n    \"\"\"\n    Adds extra data to the main DataFrame, prioritizing existing competition data.\n    \"\"\"\n    n_samples_before = df_main[target].notnull().sum()\n\n    merged_df = pd.merge(df_main, df_extra, on='SMILES', how='left', suffixes=('', '_extra'))\n    df_main[target] = merged_df[target].fillna(merged_df[f'{target}_extra'])\n\n    unique_smiles_main = set(df_main['SMILES'])\n    unique_smiles_extra_only = df_extra[~df_extra['SMILES'].isin(unique_smiles_main)].copy()\n\n    df_main = pd.concat([df_main, unique_smiles_extra_only], axis=0, ignore_index=True)\n\n    n_samples_after = df_main[target].notnull().sum()\n    print(f'\\nFor target \"{target}\" added {n_samples_after-n_samples_before} new samples!')\n    print(f'New unique SMILES added: {len(unique_smiles_extra_only)}')\n\n    if f'{target}_extra' in df_main.columns:\n        df_main = df_main.drop(columns=[f'{target}_extra'])\n\n    return df_main\n\n# --- Feature Engineering ---\nUSELESS_DESCRIPTORS = {\n    'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO',\n    'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW',\n    'NumRadicalElectrons', 'SMR_VSA8', 'SlogP_VSA9', 'fr_barbitur',\n    'fr_benzodiazepine', 'fr_dihydropyridine', 'fr_epoxide', 'fr_isothiocyan',\n    'fr_lactam', 'fr_nitroso', 'fr_prisulfonamd', 'fr_thiocyan',\n    'MaxEStateIndex', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons',\n    'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Kappa1',\n    'LabuteASA', 'HeavyAtomCount', 'MolMR', 'Chi3n', 'BertzCT', 'Chi2v',\n    'Chi4n', 'HallKierAlpha', 'Chi3v', 'Chi4v', 'MinAbsPartialCharge',\n    'MinPartialCharge', 'MaxAbsPartialCharge', 'FpDensityMorgan2',\n    'FpDensityMorgan3', 'Phi', 'Kappa3', 'fr_nitrile', 'SlogP_VSA6',\n    'NumAromaticCarbocycles', 'NumAromaticRings', 'fr_benzene', 'VSA_EState6',\n    'NOCount', 'fr_C_O', 'fr_C_O_noCOO', 'fr_amide',\n    'fr_Nhpyrrole', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_COO2',\n    'fr_halogen', 'fr_diazo', 'fr_nitro_arom', 'fr_phos_ester',\n    'fr_C_O_noCOO'\n}\n\nALL_DESC_TUPLES = [(desc[0], desc[1]) for desc in Descriptors.descList if desc[0] not in USELESS_DESCRIPTORS]\nDESC_NAMES = [desc[0] for desc in ALL_DESC_TUPLES]\n\ndef compute_molecular_descriptors(mol):\n    \"\"\"\n    Computes RDKit molecular descriptors for a given RDKit molecule object.\n    Returns a list of descriptor values.\n    \"\"\"\n    if mol is None:\n        return [np.nan] * len(DESC_NAMES)\n    return [desc_func(mol) for _, desc_func in ALL_DESC_TUPLES]\n\ndef compute_graph_features_for_mol(mol):\n    \"\"\"\n    Computes graph-based features for a given RDKit molecule object.\n    Returns a dictionary of graph features.\n    \"\"\"\n    if mol is None:\n        return {'graph_diameter': 0, 'avg_shortest_path': 0, 'num_cycles': 0}\n\n    adj = rdmolops.GetAdjacencyMatrix(mol)\n    G = nx.from_numpy_array(adj)\n\n    graph_diameter = 0\n    avg_shortest_path = 0\n    if nx.is_connected(G) and len(G) > 1:\n        try:\n            graph_diameter = nx.diameter(G)\n            avg_shortest_path = nx.average_shortest_path_length(G)\n        except nx.NetworkXError:\n            pass\n\n    num_cycles = len(list(nx.cycle_basis(G)))\n\n    return {\n        'graph_diameter': graph_diameter,\n        'avg_shortest_path': avg_shortest_path,\n        'num_cycles': num_cycles\n    }\n\ndef generate_features(df):\n    \"\"\"\n    Generates all molecular and graph features for a DataFrame.\n    \"\"\"\n    mols = [Chem.MolFromSmiles(smi) for smi in df['SMILES']]\n\n    descriptors_list = [compute_molecular_descriptors(mol) for mol in mols]\n    descriptors_df = pd.DataFrame(descriptors_list, columns=DESC_NAMES)\n\n    graph_features_list = [compute_graph_features_for_mol(mol) for mol in mols]\n    graph_features_df = pd.DataFrame(graph_features_list)\n\n    result = pd.concat([descriptors_df, graph_features_df], axis=1)\n    result = result.replace([-np.inf, np.inf], np.nan)\n    return result\n\n# --- TabTransformer Model Definition ---\nclass Embeddings(nn.Module):\n    def __init__(self, categories, embedding_dim):\n        super().__init__()\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(num_unique + 1, embedding_dim) \n            for num_unique in categories\n        ])\n        for embedding in self.embeddings:\n            nn.init.xavier_uniform_(embedding.weight)\n\n    def forward(self, x):\n        return torch.cat([emb(x[:, i]) for i, emb in enumerate(self.embeddings)], 1)\n\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, ff_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(ff_hidden_dim, embed_dim)\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        attn_output, _ = self.attn(x, x, x)\n        x = self.norm1(x + self.dropout(attn_output)) \n\n        ff_output = self.ff(x)\n        x = self.norm2(x + self.dropout(ff_output)) \n        return x\n\nclass TabTransformer(nn.Module):\n    def __init__(self,\n                 categories, \n                 num_continuous, \n                 embedding_dim,\n                 num_heads, \n                 num_encoders, \n                 ff_hidden_dim, \n                 dropout=0.1,\n                 output_dim=1): \n        super().__init__()\n\n        self.categorical_columns = len(categories)\n        self.continuous_columns = num_continuous\n        self.embedding_dim = embedding_dim\n\n        # Categorical embeddings\n        self.categorical_embeddings = Embeddings(categories, embedding_dim)\n\n        # Transformer encoder blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoderBlock(embedding_dim, num_heads, ff_hidden_dim, dropout)\n            for _ in range(num_encoders)\n        ])\n\n        # Final projection for combined features\n        total_feature_dim = self.categorical_columns * embedding_dim + self.continuous_columns\n        self.mlp_head = nn.Sequential(\n            nn.Linear(total_feature_dim, ff_hidden_dim), \n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(ff_hidden_dim, output_dim)\n        )\n\n    def forward(self, x_cat, x_cont):\n\n        # Process categorical features\n        if self.categorical_columns > 0:\n            x_cat_embedded = self.categorical_embeddings(x_cat) \n            x_cat_embedded = x_cat_embedded.view(x_cat_embedded.size(0), self.categorical_columns, self.embedding_dim) \n\n            # Apply Transformer blocks\n            for block in self.transformer_blocks:\n                x_cat_embedded = block(x_cat_embedded)\n\n            # Flatten the output of transformer for concatenation\n            x_cat_processed = x_cat_embedded.view(x_cat_embedded.size(0), -1)\n        else:\n            x_cat_processed = torch.empty(x_cont.size(0), 0).to(CFG.DEVICE)\n\n        # Concatenate with continuous features\n        combined_features = torch.cat([x_cat_processed, x_cont], dim=1)\n\n        # Final MLP head\n        return self.mlp_head(combined_features).squeeze()\n\n\n# --- Custom Dataset for PyTorch DataLoader ---\nclass TabularDataset(Dataset):\n    def __init__(self, df, categorical_cols, continuous_cols, target_col=None):\n        self.categorical_cols = categorical_cols\n        self.continuous_cols = continuous_cols\n        self.target_col = target_col\n\n        self.categorical_data = torch.tensor(df[categorical_cols].values, dtype=torch.long)\n        self.continuous_data = torch.tensor(df[continuous_cols].values, dtype=torch.float32)\n\n        if target_col and target_col in df.columns:\n            self.targets = torch.tensor(df[target_col].values, dtype=torch.float32)\n        else:\n            self.targets = None\n\n    def __len__(self):\n        return len(self.categorical_data)\n\n    def __getitem__(self, idx):\n        cat_features = self.categorical_data[idx]\n        cont_features = self.continuous_data[idx]\n        if self.targets is not None:\n            return cat_features, cont_features, self.targets[idx]\n        return cat_features, cont_features\n\n\n# --- Evaluation Metric ---\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculates Mean Absolute Error (MAE).\n    \"\"\"\n    return np.mean(np.abs(y_true - y_pred))\n\n# --- TabTransformer Model Training and Prediction ---\ndef train_and_predict_tabtransformer(train_df_full, test_df_full):\n    \"\"\"\n    Trains TabTransformer models for each target and generates OOF and test predictions.\n    \"\"\"\n    test_preds_df = test_df_full[['id', 'SMILES']].copy()\n    for target_col in CFG.TARGETS:\n        test_preds_df[target_col] = 0.0\n\n    oof_preds_df = train_df_full[['id'] + CFG.TARGETS].copy()\n    for target_col in CFG.TARGETS:\n        oof_preds_df[target_col] = np.nan\n\n    \n\n    CATEGORICAL_THRESHOLD = 20 \n\n    all_features = [col for col in train_df_full.columns if col not in ['id', 'SMILES'] + CFG.TARGETS]\n\n    # Pre-process features for TabTransformer\n    # We need to map categorical features to integer indices\n    # And scale continuous features\n\n    # Store encoders and scalers\n    categorical_encoders = {}\n    scalers = {}\n\n    # Determine which features are categorical and which are continuous based on *entire* dataset\n    global_categorical_cols = []\n    global_continuous_cols = []  \n\n    for col in all_features:\n        # Check if the feature exists in both train and test and is not entirely NaN\n        if col in train_df_full.columns and col in test_df_full.columns and \\\n           not (train_df_full[col].isnull().all() and test_df_full[col].isnull().all()):\n\n            combined_unique_count = pd.concat([train_df_full[col], test_df_full[col]]).nunique(dropna=False)\n\n            if combined_unique_count <= CATEGORICAL_THRESHOLD:\n                global_categorical_cols.append(col)\n            else:\n                global_continuous_cols.append(col)\n\n    # Fit LabelEncoders for categorical features on the combined data\n    for col in global_categorical_cols:\n        le = LabelEncoder() \n        combined_values = pd.concat([train_df_full[col].astype(str), test_df_full[col].astype(str)])\n        le.fit(combined_values)\n        categorical_encoders[col] = le\n        train_df_full[col] = le.transform(train_df_full[col].astype(str))\n        test_df_full[col] = le.transform(test_df_full[col].astype(str))\n\n    # Fit StandardScaler for continuous features on the training data\n    for col in global_continuous_cols:\n        scaler = StandardScaler()\n        train_mean = train_df_full[col].mean()\n        train_df_full[col].fillna(train_mean, inplace=True)\n        test_df_full[col].fillna(train_mean, inplace=True) \n\n        train_df_full[col] = scaler.fit_transform(train_df_full[[col]])\n        test_df_full[col] = scaler.transform(test_df_full[[col]])\n        scalers[col] = scaler\n\n    print(f\"\\nIdentified {len(global_categorical_cols)} categorical features and {len(global_continuous_cols)} continuous features.\")\n\n\n    for target in CFG.TARGETS:\n    # for target in ['Rg']:        \n        print(f'\\n\\nTARGET: {target}')\n\n        train_part = train_df_full[train_df_full[target].notnull()].reset_index(drop=True)\n\n        oof_tab_target = np.zeros(len(train_part))      \n        scores = []\n\n        kf = KFold(n_splits=CFG.FOLDS, shuffle=True, random_state=CFG.SEED)\n\n        categories_counts = [len(categorical_encoders[col].classes_) for col in global_categorical_cols]\n\n        for i, (trn_idx, val_idx) in enumerate(kf.split(train_part)):\n            print(f\"\\n--- Fold {i+1} ---\")\n\n            x_trn_cat = train_part.loc[trn_idx, global_categorical_cols]\n            x_trn_cont = train_part.loc[trn_idx, global_continuous_cols]\n            y_trn = train_part.loc[trn_idx, target]\n\n            x_val_cat = train_part.loc[val_idx, global_categorical_cols]\n            x_val_cont = train_part.loc[val_idx, global_continuous_cols]\n            y_val = train_part.loc[val_idx, target]\n\n            # Create PyTorch Datasets and DataLoaders\n            train_dataset = TabularDataset(train_part.loc[trn_idx], global_categorical_cols, global_continuous_cols, target)\n            val_dataset = TabularDataset(train_part.loc[val_idx], global_categorical_cols, global_continuous_cols, target)\n            test_dataset = TabularDataset(test_df_full, global_categorical_cols, global_continuous_cols) \n\n            train_loader = DataLoader(train_dataset, batch_size=CFG.TAB_BATCH_SIZE, shuffle=True)\n            val_loader = DataLoader(val_dataset, batch_size=CFG.TAB_BATCH_SIZE, shuffle=False)\n            test_loader = DataLoader(test_dataset, batch_size=CFG.TAB_BATCH_SIZE, shuffle=False)\n\n\n            # EnhancedFTTransformer\n            model = EnhancedFTTransformer(\n                categories=categories_counts,\n                num_continuous=len(global_continuous_cols),\n                embedding_dim=CFG.EMBEDDING_DIM,\n                num_heads=CFG.NUM_HEADS,\n                num_layers=CFG.NUM_LAYERS,\n                ff_hidden_dim=CFG.FF_HIDDEN_DIM,\n                dropout=CFG.DROPOUT,\n                output_dim=1\n            ).to(CFG.DEVICE)\n\n            if target == 'FFV':\n                CFG.TAB_LEARNING_RATE = 1e-3 #5e-4\n                CFG.TAB_EPOCHS = 100\n                CFG.LR_DECAY_RATE = 0.99999        \n            if target == 'Rg':\n                CFG.TAB_LEARNING_RATE = 1e-3 #5e-4\n                CFG.TAB_EPOCHS = 80\n                CFG.LR_DECAY_RATE = 0.92588\n\n            optimizer = torch.optim.Adam(model.parameters(), lr=CFG.TAB_LEARNING_RATE,weight_decay=CFG.WEIGHT_DECAY)\n            lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: CFG.LR_DECAY_RATE ** epoch)\n            criterion = nn.L1Loss() \n\n            best_val_loss = float('inf')\n            patience_counter = 0\n\n            for epoch in range(CFG.TAB_EPOCHS):\n                model.train()\n                total_loss = 0\n                for cat_batch, cont_batch, target_batch in train_loader:\n                    cat_batch, cont_batch, target_batch = cat_batch.to(CFG.DEVICE), cont_batch.to(CFG.DEVICE), target_batch.to(CFG.DEVICE)\n\n                    optimizer.zero_grad()\n                    out = model(cat_batch, cont_batch)\n                    loss = criterion(out,target_batch)\n                    # loss = criterion(out.flatten(),target_batch.repeat_interleave(CFG.TAB_BATCH_SIZE))\n                    # print(out.flatten().shape,target_batch.repeat_interleave(CFG.TAB_BATCH_SIZE).shape)\n                    # print(loss.item())\n                    loss.backward()\n                    optimizer.step()\n                    total_loss += loss.item() * len(target_batch) \n                lr_scheduler.step()\n\n                # Validation phase\n                model.eval()\n                val_preds_list = []\n                val_true_list = []\n                val_loss_epoch = 0\n                with torch.no_grad():\n                    for cat_batch, cont_batch, target_batch in val_loader:\n                        cat_batch, cont_batch, target_batch = cat_batch.to(CFG.DEVICE), cont_batch.to(CFG.DEVICE), target_batch.to(CFG.DEVICE)\n                        out = model(cat_batch, cont_batch)\n                        # print(f\" out shape {out.shape}\")\n                        # print(f\"out mean shape {out.mean(1)}\")\n                        # loss = criterion(out.flatten(),target_batch.repeat_interleave(CFG.TAB_BATCH_SIZE))\n                        loss = criterion(out,target_batch)\n                        val_loss_epoch += loss.item() * len(target_batch)\n                        # val_preds_list.extend(out.sum(1).cpu().tolist())\n                        val_preds_list.extend(out.cpu().tolist())\n                        val_true_list.extend(target_batch.cpu().tolist())\n\n                current_val_loss = val_loss_epoch / len(val_dataset)\n                current_val_mae = mae(np.array(val_true_list), np.array(val_preds_list))\n\n                if current_val_loss < best_val_loss:\n                    best_val_loss = current_val_loss\n                    patience_counter = 0\n                    torch.save(model.state_dict(), f'./tabtransformer_model_{target}_fold_{i+1}_best.pt')\n                else:\n                    patience_counter += 1\n                    if patience_counter >= CFG.TAB_EARLY_STOPPING_PATIENCE:\n                        print(f\"Early stopping at epoch {epoch+1} for fold {i+1}, target {target}\")\n                        break\n\n                if epoch % 10 == 0 or epoch == CFG.TAB_EPOCHS -1:\n                    print(f\"Epoch {epoch+1}/{CFG.TAB_EPOCHS}, Train Loss: {total_loss/len(train_dataset):.5f}, Val Loss: {current_val_loss:.5f}, Val MAE: {current_val_mae:.5f}\")\n\n            # Load the best model for OOF and test predictions\n            model.load_state_dict(torch.load(f'./tabtransformer_model_{target}_fold_{i+1}_best.pt'))\n            model.eval()\n\n            # OOF predictions\n            oof_fold_preds = []\n            with torch.no_grad():\n                for cat_batch, cont_batch, _ in val_loader: \n                    cat_batch, cont_batch = cat_batch.to(CFG.DEVICE), cont_batch.to(CFG.DEVICE)\n                    out = model(cat_batch, cont_batch)\n                    oof_fold_preds.extend(out.cpu().tolist())\n                    # oof_fold_preds.extend(out.mean(1).cpu().tolist())\n                    # print(oof_fold_preds)\n\n            # Assign OOF predictions back to the full OOF dataframe\n            oof_tab_target[val_idx] = np.array(oof_fold_preds)\n            # train.loc[train[target].notnull(), f'{target}_pred'] = np.concatenate(oof_fold_preds)\n            # train_df_full.loc[val_idx, f'{target}_pred'] = oof_fold_preds\n\n            fold_score = mae(np.array(val_true_list), np.array(oof_fold_preds))\n            scores.append(fold_score)\n            print(f'Final MAE for Fold {i+1}: {np.round(fold_score, 5)}')\n\n            # Test predictions\n            fold_test_preds = []\n            with torch.no_grad():\n                for cat_batch, cont_batch in test_loader:\n                    cat_batch, cont_batch = cat_batch.to(CFG.DEVICE), cont_batch.to(CFG.DEVICE)\n                    out = model(cat_batch, cont_batch)\n                    # fold_test_preds.extend(out.mean(1).squeeze(1).cpu().tolist())\n                    fold_test_preds.extend(out.cpu().tolist())\n\n                    # print(np.concatenate(fold_test_preds).shape)\n            test_preds_df[target] += np.array(fold_test_preds) / CFG.FOLDS      \n\n        # train.loc[train[target].notnull(), f'{target}_pred']  = oof_preds_df[target].values   \n        train_df_full.loc[train_df_full[target].notnull(),f'{target}_pred'] = oof_tab_target\n\n        print(f'\\nMean MAE for {target}: {np.mean(scores):.5f}')\n        print(f'Std MAE for {target}: {np.std(scores):.5f}')\n        print('-'*30)\n\n    return oof_preds_df, test_preds_df, train_df_full\n\n# --- Weighted Mean Absolute Error (WMAE) Metric ---\nMINMAX_DICT = {\n    'Tg': [-148.0297376, 472.25],\n    'FFV': [0.2269924, 0.77709707],\n    'Tc': [0.0465, 0.524],\n    'Density': [0.748691234, 1.840998909],\n    'Rg': [9.7283551, 34.672905605],\n}\n\ndef scaling_error(labels, preds, property_name):\n    \"\"\"\n    Calculates the scaled absolute error for a given property.\n    \"\"\"\n    error = np.abs(labels - preds)\n    min_val, max_val = MINMAX_DICT[property_name]\n    label_range = max_val - min_val\n    return np.mean(error / (label_range + 1e-9))\n\ndef get_property_weights(labels_df):\n    \"\"\"\n    Calculates weights for each property based on the number of non-null labels.\n    \"\"\"\n    property_weight = []\n    for property_name in MINMAX_DICT.keys():\n        valid_num = np.sum(labels_df[property_name] != CFG.NULL_FOR_SUBMISSION)\n        property_weight.append(valid_num)\n    property_weight = np.array(property_weight)\n    property_weight = np.sqrt(1 / (property_weight + 1e-9))\n    return (property_weight / np.sum(property_weight)) * len(property_weight)\n\ndef wmae_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Calculates the Weighted Mean Absolute Error (WMAE) score.\n    \"\"\"\n    chemical_properties = list(MINMAX_DICT.keys())\n    property_maes = []\n\n    solution_aligned = solution.set_index(row_id_column_name).reindex(submission[row_id_column_name]).reset_index()\n\n    property_weights = get_property_weights(solution_aligned[chemical_properties])\n\n    for i, property_name in enumerate(chemical_properties):\n        is_labeled = solution_aligned[property_name] != CFG.NULL_FOR_SUBMISSION\n\n        if np.any(is_labeled):\n            mae_val = scaling_error(solution_aligned.loc[is_labeled, property_name],\n                                     submission.loc[is_labeled, property_name],\n                                     property_name)\n            property_maes.append(mae_val)\n        else:\n            property_maes.append(0.0)\n\n    if not property_maes or np.sum(property_weights) == 0:\n        raise RuntimeError('No labels or all property weights are zero. Cannot calculate WMAE.')\n\n    return float(np.average(property_maes, weights=property_weights))\n\n# --- Main Execution Flow ---\nif __name__ == '__main__':\n    print(f\"Using device: {CFG.DEVICE}\")\n\n    # Load Data\n    train_df, test_df = load_data()\n\n    # Canonicalize SMILES\n    print(\"Canonicalizing SMILES strings...\")\n    train_df['SMILES'] = train_df['SMILES'].apply(make_smile_canonical)\n    test_df['SMILES'] = test_df['SMILES'].apply(make_smile_canonical)\n    initial_train_rows = len(train_df)\n    initial_test_rows = len(test_df)\n    train_df = train_df.dropna(subset=['SMILES']).reset_index(drop=True)\n    test_df = test_df.dropna(subset=['SMILES']).reset_index(drop=True)\n    print(f\"Dropped {initial_train_rows - len(train_df)} rows from train due to invalid SMILES.\")\n    print(f\"Dropped {initial_test_rows - len(test_df)} rows from test due to invalid SMILES.\")\n    print(\"SMILES canonicalization complete.\")\n\n    # Load and Add Extra Data\n    print(\"\\nLoading and integrating extra data...\")\n    data_tc, data_tg2, data_tg3, data_dnst = load_and_preprocess_extra_data()\n\n    train_df = add_extra_data(train_df, data_tc, 'Tc')\n    train_df = add_extra_data(train_df, data_tg2, 'Tg')\n    train_df = add_extra_data(train_df, data_tg3, 'Tg')\n    train_df = add_extra_data(train_df, data_dnst, 'Density')\n\n    print('\\n--- SMILES count for training after extra data addition ---')\n    for t in CFG.TARGETS:\n        print(f'\"{t}\": {train_df[t].notnull().sum()}')\n\n    # Feature Engineering (Generate RDKit and Graph features as tabular input)\n    print(\"\\nGenerating molecular and graph features for train and test sets...\")\n    train_features = generate_features(train_df)\n    test_features = generate_features(test_df)\n\n    # Concatenate features back to the main dataframes for unified processing\n    train_df_full_features = pd.concat([train_df, train_features], axis=1)\n    test_df_full_features = pd.concat([test_df, test_features], axis=1)\n    print(\"Feature generation complete.\")\n\n    # Clean up memory\n    del train_features, test_features\n    gc.collect()\n\n    print(\"\\nStarting TabTransformer training and prediction...\")\n    oof_preds_df, test_preds_df,train_df_full = train_and_predict_tabtransformer(train_df_full_features, test_df_full_features)\n    print(\"TabTransformer training and prediction complete.\")\n\n    # Calculate WMAE Score (Out-of-Fold)\n    print(f\"\\n--- Overall WMAE Score (Out-Of-Fold) ---\")\n\n    tr_solution = train_df_full[['id'] + CFG.TARGETS]\n    tr_submission = train_df_full[['id'] + [t + '_pred' for t in CFG.TARGETS]]\n    tr_submission.columns = ['id'] + CFG.TARGETS\n    print(\"*\"*25 +\" FINAL SCORE WAME \"+\"*\"*25)\n    print(f\"wMAE: {round(wmae_score(tr_solution, tr_submission, row_id_column_name='id'), 5)}\")\n    print(\"*\"*50)\n\n    # Handle Overlapping SMILES between Train and Test for Submission\n    # print(\"\\nHandling overlapping SMILES for final submission...\")\n    # for target in CFG.TARGETS:\n    #     train_smiles_known = train_df[train_df[target].notnull()][['SMILES', target]].drop_duplicates(subset=['SMILES']).copy()\n\n    #     merged_test = pd.merge(test_preds_df[['id', 'SMILES', target]], train_smiles_known, on='SMILES', how='left', suffixes=('_pred', '_known'))\n\n    #     test_preds_df[target] = merged_test[f'{target}_known'].fillna(merged_test[f'{target}_pred'])\n\n    # print(\"Overlapping SMILES handled.\")\n\n    # Create Submission File\n    final_submission_df = test_preds_df[['id'] + CFG.TARGETS].copy()\n    final_submission_df.to_csv('submission3.csv', index=False)\n    print(f\"\\nSubmission file 'submission.csv' created successfully. ðŸŽ‰\")\n    print(\"Submission Head:\")\n    print(final_submission_df.head())\n\n    # Final cleanup\n    del train_df, test_df, oof_preds_df, test_preds_df, data_tc, data_tg2, data_tg3, data_dnst\n    del train_df_full_features, test_df_full_features\n    gc.collect()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-21T18:34:32.926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# è¯»å–ä¸¤ä¸ªæ¨¡åž‹çš„é¢„æµ‹ç»“æžœ\nsubmission_xgb = pd.read_csv('submission1.csv')\nsubmission_bert = pd.read_csv('submission2.csv')\nsubmission_trans = pd.read_csv('submission3.csv')\n\n# æ£€æŸ¥IDé¡ºåºå’Œåˆ—åæ˜¯å¦ä¸€è‡´\n# assert (submission_xgb['id'] == submission_bert['id']).all(), \"IDé¡ºåºä¸ä¸€è‡´ï¼\"\n\n# ä½ å¯ä»¥è®¾ç½®æ¯ä¸ªæ¨¡åž‹çš„æƒé‡ï¼Œæ¯”å¦‚éƒ½è®¾0.5å°±æ˜¯ç®€å•å¹³å‡\nw_xgb = [0.55, 0.65, 0.55, 0.55, 0.55]\nw_bert = [0.325, 0.3, 0.3, 0.325, 0.325]\nw_trans = [0.125, 0.05, 0.15, 0.125, 0.125]\n\ntargets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# èžåˆ\nsubmission_fused = submission_xgb.copy()\nfor i in range(0,5):\n    submission_fused[targets[i]] = w_xgb[i] * submission_xgb[targets[i]] + w_bert[i] * submission_bert[targets[i]] +w_trans[i] * submission_trans[targets[i]]\nsubmission_fused['Tg'] += 40.00\n# è¾“å‡ºèžåˆåŽçš„ç»“æžœ\nsubmission_fused.to_csv('submission.csv', index=False)\nsubmission_fused.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T22:03:03.503834Z","iopub.execute_input":"2025-08-24T22:03:03.504611Z","iopub.status.idle":"2025-08-24T22:03:03.567249Z","shell.execute_reply.started":"2025-08-24T22:03:03.50457Z","shell.execute_reply":"2025-08-24T22:03:03.566548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}